{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro-hw2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# <center><u> CP322: Assignment 2 – Winter 2023 </u> </center>\n",
    "# <center><u>  Due on  February 16, 2023 (Before 11:59 PM) </u> </center>\n",
    " \n",
    " This is a **group (of 2)** assignment, and we will practice the concepts to build classification models, regularization and cross-validation. \n",
    " \n",
    " For this assignment, you must use Python language. You will use this jupyter notebook to write your code without errors. If your code will not run, then you will score zero. Therefore, ensure you have removed all syntax errors from your code and not changing the variable names. Gradescope platform would be used to upload the assignments for grading and autograded question's feedback will be visible to you. The link to the Gradescope assignment is available on Myls course page. \n",
    " \n",
    " For submission, drag and drop your code file(s) into Gradescope. Make sure that your file name should be as suggested in the assignment, using a different name may score zero.\n",
    " \n",
    " - Please note that the submitted code will be checked for plagiarism. By submitting this zip file, you would confirm that you have not received unauthorized assistance in preparing the assignment. You also confirm that you are aware of course policies for submitted work. \n",
    " \n",
    " - Marks will be deducted from any questions where these requirements are not met.\n",
    " \n",
    " - Multiple attempts will be allowed, and only your last submission before the deadline will be graded. Instructor reserves the right to take off points for not following the directions.\n",
    " \n",
    "<b>Warning:</b> Follow the assignment instructions to the letter in terms of the variable names and function names, as this assignment will be auto-graded. If anything is not as per the description, the auto-grading fails, and your assignment will be given a Zero mark.\n",
    "\n",
    "Note: \n",
    "- For each question in the assignment, please write down your answer in the answer cell(s) right below the question. \n",
    "- It is helpful to have extra cells breaking down the process towards reaching your final answer. If you happen to create new cells below your answer to run codes, **NEVER** add cells between a question cell and the answer cell below it. It will cause errors when autograder run.\n",
    "- Finally, unless it is stated otherwise, try to avoid using python for loops or list comprehensions.  The majority of this part of assignment can be done using builtin commands in Pandas, numpy, searborn, matplotlib, and sklearn.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Python Packages\n",
    "\n",
    "If any of the package is not available then install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to suppress all FutureWarnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# for data manipulation\n",
    "import pandas as pd, numpy as np, random\n",
    "# Set a Random State value\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "#for graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#models to run\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "#train_test_split\n",
    "from sklearn.model_selection import train_test_split,StratifiedShuffleSplit, GridSearchCV\n",
    "\n",
    "#for cycle\n",
    "from itertools import cycle\n",
    "\n",
    "#metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix,precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loading",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Loading in the Data\n",
    "\n",
    "Network intrusion detection (NID) is a method of protecting computer networks by identifying and preventing unauthorized access, misuse, modification, or denial of service. NID software monitors network traffic and looks for indicators of malicious activity, and can alert network administrators to suspicious activity and take action to block or isolate the source of the intrusion. In this assignment, we will create a network intrusion detection system, a predictive model that can differentiate between malicious connections, known as intrusions or attacks, and legitimate connections. The dataset used for building this model includes a diverse range of intrusions simulated in a military network setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:17:42.181245Z",
     "start_time": "2019-04-03T20:17:41.343927Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "fetch-data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# read raw data\n",
    "train_data_original= pd.read_csv('https://raw.githubusercontent.com/sukhjitsehra/datasets/master/CP322/id/train.csv')\n",
    "test_data_original = pd.read_csv('https://raw.githubusercontent.com/sukhjitsehra/datasets/master/CP322/id/test.csv')\n",
    "test_data_original.columns = train_data_original.columns\n",
    "train_data_original.label.value_counts().keys()\n",
    "test_data_original.label.value_counts().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "train_data = train_data_original\n",
    "test_data = test_data_original"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-34476156ed73b800",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 1a\n",
    "An important step in dealing with datasets is to clean the available data before using it for building models. Some important steps involved in the data cleaning process are removing/imputing NULL values, removing duplicates from the dataset and remove \". \" from `labels` in the label column of both `train_data` and `test_data`.\n",
    "\n",
    "First, let's check if our data contains any missing values. Fill in the cell below to print the number of NaN values in each column. If there are NaN values, replace them with appropriate filler values (i.e., NaN values should be replaced with 0 value). Print the number of NaN values in each column after this modification to verify that there are no NaN values left.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:17:42.203231Z",
     "start_time": "2019-04-03T20:17:42.185104Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b1fb39d9b651ca1b",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# print the count to null values\n",
    "print('Before imputation:')\n",
    "...\n",
    "...\n",
    "\n",
    "# handle the null value by imputation\n",
    "train_data = ...\n",
    "test_data = ...\n",
    "print('------------') \n",
    "\n",
    "# print the count if all null values are handled\n",
    "print('After imputation:')\n",
    "...\n",
    "...\n",
    "\n",
    "# remove the '.' at the end of labels\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 1b\n",
    "\n",
    "In the cell below,  drop the duplicate rows in out `train_data` and `test_data`. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:17:42.247245Z",
     "start_time": "2019-04-03T20:17:42.228451Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "q1-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1c\n",
    "\n",
    "Exploratory data analysis (EDA) is an approach for analyzing the dataset to summarize their main characteristics, often with visual methods. A statistical model may or may not be used, but primarily EDA is for seeing what the data can tell us beyond the formal modeling.\n",
    "\n",
    "Now, create a plot of distribution of labels from `train_data` in the following code chunk and also write down the at least two observations about the dataset.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1c\n",
    "manual: true\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_color_codes()\n",
    "fig, ax1 = plt.subplots(1,1, figsize = (18,6))\n",
    "...\n",
    "plt.xticks(rotation=30)\n",
    "plt.draw()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "....\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "\n",
    "### Question 1d\n",
    "\n",
    "For multinomial classification, we combine some of labels into attack_type, as given below: \n",
    "\n",
    "Convert labels into attack_types.\n",
    "\n",
    "1. ['back', 'land', 'neptune', 'pod', 'smurf', 'teardrop'] -> 'dos'\n",
    "2. ['buffer_overflow', 'loadmodule','perl', 'rootkit'] -> 'utr'\n",
    "3. ['Ftp_write', 'guess_passwd', 'imap', 'multihop', 'phf', 'spy','warezclient', 'warezmaster'] -> 'rtl'\n",
    "4. ['satan', 'ipsweep', 'nmap', 'portsweep'] -> 'probes'\n",
    "\n",
    "Complete the following function to group the labels, and update both `train_data` and `test_data`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1d\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# combining labels \n",
    "def label_grouping(label):\n",
    "    \"\"\"Group the label together. Returns new label.\"\"\"\n",
    "    ...\n",
    "        return 'dos'\n",
    "    ...\n",
    "        return 'utr'\n",
    "    ...\n",
    "        return 'rtl'\n",
    "    ...\n",
    "        return 'probes'\n",
    "        return 'normal'\n",
    "    ...\n",
    "        return 'others'\n",
    "\n",
    "# Update the data \n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1d\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "\n",
    "### Question 1e \n",
    "\n",
    "Explore the data graphically in order to investigate the distribution of variables with other features from `train_data`. Describe your findings, while creating the following:\n",
    "\n",
    "a) Bar plot of distribution of attack-type labels (normal, dos, utr, rtl, probes).\n",
    "\n",
    "b) Distribution of features:\n",
    "    \n",
    "    1. Categorical features : Use `sns.countplot` to understand prominent values of a feature in each class.\n",
    "    \n",
    "    2. Continuous features : Use `sns.boxplot` to understand distribution of a feature in each class.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1e\n",
    "manual: true\n",
    "points: 6\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# a) distribution of label_attack_type\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1, figsize = (18,6))\n",
    "\n",
    "...\n",
    "plt.xticks(rotation=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#b i) distribution of categorical variables with 'label_attack_type'\n",
    "sns.set()\n",
    "categorical_cols = ['protocol_type','flag','land','logged_in','is_host_login','is_guest_login']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    plt.figure()\n",
    "    sns.countplot(x=col, hue=\"label_attack_type\",data=train_data, palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#b ii) \n",
    "for column in train_data.columns:\n",
    "    ...\n",
    "        plt.figure()\n",
    "        ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-78513403ef52a957",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 2a\n",
    "\n",
    "a) Complete the function to convert data labels to numerical values, e.g. dos=1, utr=2, rtl=3, probes=4, normal=0, and update both `train_data` and `test_data` by creating a new column `label_encoding`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_label_encoding(label):\n",
    "    \"\"\"Create numeric labels. Return the label.\"\"\"\n",
    "    if label = ...\n",
    "        return 1\n",
    "    elif label = ...\n",
    "        return 2\n",
    "    elif label = ...\n",
    "        return 3\n",
    "    elif label = ...\n",
    "        return 4\n",
    "    elif label = ...\n",
    "        return 5\n",
    "    ...\n",
    "        return 0\n",
    "\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "test_data.label_encoding.value_counts()[1] == 21720\n",
    "train_data['label_encoding'].value_counts()[0] ==  87814"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-78513403ef52a957",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 2b\n",
    "\n",
    "One-hot encoding of categorical features in the `flag` and `protocol_type` categorical variables for both `train_data` and `test_data`. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#one hot encoding of categorical variables using get_dummies function and add prefix = 'prefix = 'flag'\n",
    "flag_encoding_test = ...\n",
    "\n",
    "#one hot encoding of categorical variables using get_dummies function and add prefix = 'protocol' \n",
    "protocol_encoding_test = ...\n",
    "\n",
    "# concat the dummies with original dataframe\n",
    "test_data = ...\n",
    "\n",
    "#one hot encoding of categorical variables\n",
    "flag_encoding_train = ...\n",
    "protocol_encoding_train = ...\n",
    "\n",
    "# concat with original dataframe\n",
    "train_data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2c\n",
    "\n",
    "Separate the predictors and target variables in `train_data` and `test_data` datasets, our target variable is `label_encoding` and predictors except the following:\n",
    "\n",
    "    'label', 'label_attack_type', 'index', 'protocol_type','flag','service','is_host_login','label_encoding','count','same_srv_rate','diff_srv_rate','src_bytes','flag_SF', 'dst_host_same_srv_rate','dst_host_srv_count','dst_bytes','dst_host_srv_serror_rate','dst_host_diff_srv_rate','dst_host_serror_rate','srv_serror_rate','flag_S0','serror_rate','logged_in','dst_host_same_src_port_rate','dst_host_count'\n",
    "\n",
    "Note: Drop the unwanted columns from the `train_data` and `test_data` and create variables `X_train, y_train, X_test, y_test`. Make, you are not making any changes to the datasets.\n",
    "\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2c\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# select the required predictors and remove unwanted.\n",
    "...\n",
    "X_train = ...\n",
    "y_train = ...\n",
    "X_test = ...\n",
    "y_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Classifiers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have discussed that evaluating accuracy on the training set may provide a misleading measure alone. Accuracy on the training set doesn't always translate to accuracy in the real world (on the test set). In future parts of this analysis, we will hold out some of our data for model validation and comparison.\n",
    "\n",
    "False positives and false negatives may be of differing importance, leading us to consider more ways of evaluating a classifier, in addition to overall accuracy:\n",
    "\n",
    "- **Precision**: $\\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$\n",
    "\n",
    "- **Recall**: $\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$ \n",
    "\n",
    "- **False-alarm rate**: $\\frac{\\text{FP}}{\\text{FP} + \\text{TN}}$\n",
    "\n",
    "The two graphics below may help you understand precision and recall visually:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png\" width=\"500px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2d\n",
    "\n",
    "Write evaluation function to calculate accuracy, precision, recall, f1 score, kappa, confusion matrix. Call it ```get_performance_metrics(y_test, model_predictions, model_predictions_probability)```.\n",
    "\n",
    "Note: you can use functions from sklearn.metrics for this question.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2d\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_performance_metrics(y_test, model_predictions, model_predictions_probability):\n",
    "    \"\"\"Calculate accuracy, precision, recall, f1-score, and kappa score. Returns: Dictionary of parameters\"\"\" \n",
    "    model_accuracy = ...\n",
    "    ...\n",
    "    model_kappa = ...\n",
    "\n",
    "    # Confusion matrix\n",
    "    model_confusion_matrix = ...\n",
    "\n",
    "    # Return as dictionary\n",
    "    return {'Model_Accuracy': model_accuracy, 'Model_Precision': model_precision, 'Model_Recall': model_recall, 'Model_F1_Score': model_f1, \\\n",
    "         'Model_Kappa': model_kappa, 'Confusion_Matrix': model_confusion_matrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2d\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3a\n",
    "\n",
    "Complete a function in the code chunk below to fit classification models and its take `model_name, X_train, y_train, X_test` as input parameters and return back the predictions and prediction probabilities. Using this function:\n",
    "\n",
    " a) Fit LDA on the training data using the predictor variables in (X_train) dataset to predict the `label` class (i.e. using X_test data). \n",
    " \n",
    " b) Fit QDA on the training data using the predictor variables in (X_train) dataset to predict the `label` class (i.e. using X_test data).  \n",
    "\n",
    " c) Fit logistic regression on the training data using the predictor variables in (X_train) dataset to predict the `label` class (i.e. using X_test data). \n",
    "\n",
    " d) Fit naive Bayes on the training data using the predictor variables in (X_train) dataset to predict the `label` class (i.e. using X_test data). \n",
    "\n",
    " e) Fit KNN on the training data, with K = 5, using the predictor variables in (X_train) dataset to predict the `label` class (i.e. using X_test data). \n",
    "\n",
    " f) Fit a decision tree on the training data using the predictor variables in (X_train) dataset to predict the `label` class (i.e. using X_test data). \n",
    "\n",
    "Note: Remember to random_state = 42 or constant RANDOM_STATE for results reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def model_fit(model, X_train, y_train, X_test):\n",
    "    \"\"\"Fit classification model using sklearn library. Returns: model predictions and probabilities\"\"\"\n",
    "    y_pred = ...\n",
    "    y_pred_proba = ...\n",
    "    return y_pred, y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#a)\n",
    "fit_lda = ...\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3aa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#b) \n",
    "fit_qda = ...\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3ab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#c)\n",
    "fit_logit = ...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3ac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#d)\n",
    "fit_gnb = ...\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#e)\n",
    "fit_knn = ...\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3ae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#f) \n",
    "fit_dt = ...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3af\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 3b\n",
    "\n",
    "Use the function `get_performance_metrics(y_test, model_predictions, model_predictions_probability)` to evaluate all the models fiited in part 3a and give your judgement on the winner model. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b\n",
    "points: 6\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "lda_eval = ...\n",
    "qda_eval = ...\n",
    "logit_eval = ...\n",
    "gnb_eval = ...\n",
    "knn_eval = ...\n",
    "dt_eval = ...\n",
    "\n",
    "evaluations = [lda_eval, qda_eval, logit_eval, gnb_eval, knn_eval, dt_eval]\n",
    "model_names = ['LDA', 'QDA', 'Logistic Regression', 'Naive Bayes', 'KNN', 'Decision Tree']\n",
    "\n",
    "for i in range(len(evaluations)):\n",
    "    print(f\"Model {model_names[i]} has accuracy of {evaluations[i].get('Model_Accuracy'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write your conculsion here \n",
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Resampling: K-fold Cross-Validation\n",
    "\n",
    "A common approach in machine learning is to split a dataset into training and validation sets, often using a ratio such as 70:30 or 80:20, known as the holdout method. We have used this approach above, however, it does not ensure the validation accuracy is indicative of the model's overall performance. K-fold cross-validation addresses these concerns by dividing the data into \"k\" subsets and training the model on \"k-1\" of them and testing on the remaining one. This process is repeated \"k\" times, with each subset serving as the validation set once, providing a more robust measure of the model's accuracy.\n",
    "\n",
    "## Question 3c\n",
    "\n",
    "Complete the function below to evaluate given models using cross-validation approach. The model will take as arguments: model_name, predictors, target variable dataset, number of k-fold for cross-validation. The function returns a list containing the metrics 'Accuracy' for dataset. Use this function to find the score of models using 10-fold cross-validation.\n",
    "\n",
    "[Note: Use `train_data` (i.e. X_train and y_train) along with model name and k value to find the accuracy]\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3c\n",
    "points: 12\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def kfold_cv(model, X, y, k=5):\n",
    "    # check if the input is a dataframe\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        y = y.values\n",
    "    elif isinstance(y, pd.Series):\n",
    "        y = y.values\n",
    "    # Initialize the KFold object\n",
    "    kf = ...\n",
    "    # Initialize a list to store the accuracy scores\n",
    "    scores = []\n",
    "    # Loop through the splits\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split the data into train and test sets\n",
    "        ...\n",
    "        ...\n",
    "        # Train the model on the train set\n",
    "        ...\n",
    "        # Get the predictions for the test set\n",
    "        y_pred = ...\n",
    "        # Compute the accuracy score\n",
    "        score = ...\n",
    "        # Append the score to the list\n",
    "        ...\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Report accuracies for the models that\n",
    "k_accuracy_lda = ...\n",
    "k_accuracy_lda\n",
    "k_accuracy_qda = ...\n",
    "k_accuracy_qda\n",
    "k_accuracy_logit = ...\n",
    "k_accuracy_logit\n",
    "k_accuracy_gnb = ...\n",
    "k_accuracy_gnb\n",
    "k_accuracy_knn = ...\n",
    "k_accuracy_knn\n",
    "k_accuracy_dt = ...\n",
    "k_accuracy_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Receiver Operating Characteristic (ROC) curve \n",
    "The Receiver Operating Characteristic (ROC) curve is typically used for binary classification problems, where it plots the true positive rate (TPR) against the false positive rate (FPR). For a multiclass problem, instead of having only one ROC curve, we have as many ROC curves as the number of classes. One way to handle this is by using the one-vs-all (OvA) or one-vs-the-rest (OvR) strategy, which involves training one binary classifier per class, where the class is treated as the positive class and all other classes are combined as the negative class. Then you can plot the ROC curve for each classifier.\n",
    "\n",
    "## Question 3d \n",
    "Complete the function below to use the OvA strategy to plot ROC curves for a multiclass classification problem. This function takes as input the model_name, predictors and target variable and plot the roc curve for each class. Use this function to plot the ROC curve for a LDA model and give your interpretation of the plot.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3d\n",
    "manual: true\n",
    "points: 4\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def plot_roc_auc(model, X, y):\n",
    "    y_pred_proba = ...\n",
    "    # Compute the ROC curve and AUC for each class\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    n_classes = len(set(y))\n",
    "    # Computer the FPR and TPR for each class\n",
    "    for i in range(n_classes):\n",
    "        ...\n",
    "        ...\n",
    "\n",
    "    # Plot the ROC curve for each class\n",
    "    for i in range(n_classes):\n",
    "        ...\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the LDA model\n",
    "\n",
    "plot_roc_auc(fit_logit, X_train, y_train) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 3e\n",
    "\n",
    "Complete the function below to create a plot of accuracy vs k-fold by using the results obtained by function call `kfold_cv` function call for a particular model.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3e\n",
    "manual: true\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def plot_acc(scores):\n",
    "    \"\"\"Creates a plot of accuracy vs k-fold. Returns: None\"\"\"\n",
    "    ...\n",
    "    plt.xlabel(\"Fold\")\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    plt.title(\"K-Fold Cross Validation\")\n",
    "    plt.draw()\n",
    "\n",
    "# plot the results\n",
    "\n",
    "plot_acc(k_accuracy_lda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 4a\n",
    "\n",
    "Suppose we estimate the regression coefficients in a linear regression\n",
    "model by minimizing:\n",
    "\n",
    "$\\sum_{i=1}^{n} ( y_i - \\beta_0  - \\sum_{j=1}^{p} \\beta_j x_{ij} )^2  + \\lambda \\sum_{j=1}^{p} \\beta_j^2$\n",
    "\n",
    "for a particular value of $\\lambda$. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer.\n",
    "\n",
    "(a) As we increase λ from 0, the training RSS will:\n",
    "\n",
    "    i. Increase initially, and then eventually start decreasing in an inverted U shape.\n",
    "    ii. Decrease initially, and then eventually start increasing in a U shape.\n",
    "    iii. Steadily increase.\n",
    "    iv. Steadily decrease.\n",
    "    v. Remain constant.\n",
    "\n",
    "(b) Repeat (a) for test RSS.\n",
    "\n",
    "(c) Repeat (a) for variance.\n",
    "\n",
    "(d) Repeat (a) for (squared) bias.\n",
    "\n",
    "(e) Repeat (a) for the irreducible error.\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4a\n",
    "manual: true\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 4b\n",
    "\n",
    "Now, we will predict the number of applications received using the other variables in the College data set (URL = train_data_original= pd.read_csv('https://raw.githubusercontent.com/sukhjitsehra/datasets/master/CP322/college.csv') )\n",
    "\n",
    "a) Load the dataset; create a dummy variable for `Private` column and name it as `Private_Yes`; and then drop variables: `'Unnamed: 0','Apps','Private'` .\n",
    "\n",
    "b) Split the data set into a training set and a test set (70/30 split) using random_state=RANDOM_STATE (i.e. 42).  Fit a linear model using least squares on the training set, and report the test error obtained.\n",
    "\n",
    "c) Fit a ridge regression model on the training set, with $\\lambda$ chosen by cross-validation. Report the test error obtained.\n",
    "\n",
    "d) Fit a lasso model on the training set, with $\\lambda$ chosen by cross-validation. Report the test error obtained, along with the number of non-zero coefficient estimates.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4b\n",
    "points: 8\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#a) \n",
    "# Load data set\n",
    "college = ...\n",
    "\n",
    "# create dummy variable; drop unwanted variables; separate predictors and target variables\n",
    "dummies = pd.get_dummies(college[['Private']]) # SEE\n",
    "X = ...\n",
    "y= ...\n",
    "\n",
    "# b) \n",
    "# split into train/test\n",
    "...\n",
    "\n",
    "# fit regression model\n",
    "fit_lr = ...\n",
    "...\n",
    "\n",
    "#  get accuracy\n",
    "lr_error = ...\n",
    "\n",
    "print(lr_error)\n",
    "\n",
    "\n",
    "# c)\n",
    "# create a grid for search\n",
    "alpha = 10**np.linspace(4,-2,100)\n",
    "\n",
    "# define the model (use alphas=alpha, cv=10, normalize=True)  and fit the model using X_train and y_train\n",
    "fit_rcv = ...\n",
    "...\n",
    "\n",
    "# get accuracy and value of alpha\n",
    "rcv_error = ...\n",
    "\n",
    "print(rcv_error)\n",
    "\n",
    "print(fit_rcv.alpha_)\n",
    "\n",
    "# d)\n",
    "# use the same grid as created above to define the model (use alphas=alpha, cv=10, normalize=True, max_iter=1e5) and fit the model\n",
    "\n",
    "fit_lcv = ...\n",
    "...\n",
    "\n",
    "# get accuracy and value of alpha\n",
    "lcv_error = ...\n",
    "\n",
    "print(lcv_error)\n",
    "\n",
    "print(fit_lcv.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  }
 ],
 "metadata": {
  "history": [
   {
    "code": "train=train.reset_index(drop=True) # We must do this in order to preserve the ordering of emails to labels for words_in_texts\n\n# BEGIN SOLUTION\nsome_words = ['opportunity', 'bank', 'receive', 'dear', 'best', 'deal']\nPhi_train = words_in_texts(some_words, train['email'])\n\ndf = pd.DataFrame(data = Phi_train, columns = some_words)\ndf['label'] = train['spam']\n\nplt.figure(figsize=(12,8))\nsns.barplot(x = \"variable\", \n            y = \"value\", \n            hue = \"label\", \n            data = (df\n                    .replace({'label': \n                                {0 : 'Ham', \n                                 1 : 'Spam'}})\n                    .melt('label')\n                    .groupby(['label', 'variable'])\n                    .mean()\n                    .reset_index()))\n\nplt.xlabel('Words')\nplt.ylabel('Proportion of Emails')\nplt.legend(title = \"\")\nplt.title(\"Frequency of Words in Spam/Ham Emails\")\nplt.tight_layout()\nplt.show()\n# END SOLUTION",
    "id": "3da1bb8528f1420d8e4b01106abb0d1c",
    "idx": 33,
    "time": "2020-11-13T17:39:00.446Z",
    "type": "execution"
   },
   {
    "id": "3da1bb8528f1420d8e4b01106abb0d1c",
    "time": "2020-11-13T17:39:00.597Z",
    "type": "completion"
   },
   {
    "code": "# Run this cell to suppress all FutureWarnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)",
    "id": "bf4059a762a047a6a8e4179dc2c11815",
    "idx": 5,
    "time": "2020-11-13T17:39:15.247Z",
    "type": "execution"
   },
   {
    "code": "import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set(style = \"whitegrid\", \n        color_codes = True,\n        font_scale = 1.5)",
    "id": "679de399389b410fbd1bd229389869ba",
    "idx": 8,
    "time": "2020-11-13T17:39:15.251Z",
    "type": "execution"
   },
   {
    "code": "from utils import fetch_and_cache_gdrive\nfetch_and_cache_gdrive('1SCASpLZFKCp2zek-toR3xeKX3DZnBSyp', 'train.csv')\nfetch_and_cache_gdrive('1ZDFo9OTF96B5GP2Nzn8P8-AL7CTQXmC0', 'test.csv')\n\noriginal_training_data = pd.read_csv('data/train.csv')\ntest = pd.read_csv('data/test.csv')\n\n# Convert the emails to lower case as a first step to processing the text\noriginal_training_data['email'] = original_training_data['email'].str.lower()\ntest['email'] = test['email'].str.lower()\n\noriginal_training_data.head()",
    "id": "668f75f782c041508f9e0c8f82311ad1",
    "idx": 10,
    "time": "2020-11-13T17:39:15.256Z",
    "type": "execution"
   },
   {
    "code": "# BEGIN SOLUTION\nprint('Before imputation:')\nprint(original_training_data.isnull().sum())\noriginal_training_data = original_training_data.fillna('')\nprint('------------')\nprint('After imputation:')\nprint(original_training_data.isnull().sum())\n# END SOLUTION",
    "id": "68e38b511a2c4f5480d4eda2b0082a11",
    "idx": 12,
    "time": "2020-11-13T17:39:15.259Z",
    "type": "execution"
   },
   {
    "code": "# TEST\noriginal_training_data.isnull().sum().sum() == 0",
    "id": "7f4b00c62776443e817cd5a255aa14b9",
    "idx": 13,
    "time": "2020-11-13T17:39:15.261Z",
    "type": "execution"
   },
   {
    "code": "first_ham = original_training_data.loc[original_training_data['spam'] == 0, 'email'].iloc[0] # SOLUTION\nfirst_spam = original_training_data.loc[original_training_data['spam'] == 1, 'email'].iloc[0] # SOLUTION\nprint(first_ham)\nprint(first_spam)",
    "id": "bfb55fdfdfb1456785bb78790aad426b",
    "idx": 15,
    "time": "2020-11-13T17:39:15.264Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nlen(first_ham) > 0 and first_ham[:0] == ''",
    "id": "6345c5019a384a618017acbb080996d9",
    "idx": 16,
    "time": "2020-11-13T17:39:15.265Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nlen(first_spam) > 0 and first_spam[:0] == ''",
    "id": "ee844f20ed0a41c4b81ff495fe482bdf",
    "idx": 17,
    "time": "2020-11-13T17:39:15.268Z",
    "type": "execution"
   },
   {
    "code": "# HIDDEN TEST\noriginal_training_data.loc[original_training_data['spam'] == 0, 'email'].iloc[0] in first_ham",
    "id": "13a350f12fac4931be17235a1daf0a93",
    "idx": 18,
    "time": "2020-11-13T17:39:15.270Z",
    "type": "execution"
   },
   {
    "code": "# HIDDEN TEST\noriginal_training_data.loc[original_training_data['spam'] == 1, 'email'].iloc[0] in first_spam",
    "id": "a90683e4a73346878a06d732d7101c87",
    "idx": 19,
    "time": "2020-11-13T17:39:15.272Z",
    "type": "execution"
   },
   {
    "code": "# This creates a 90/10 train-validation split on our labeled data\n\nfrom sklearn.model_selection import train_test_split\n\ntrain, val = train_test_split(original_training_data, test_size=0.1, random_state=42)",
    "id": "7523257e508447369ca710ce27eba7b1",
    "idx": 23,
    "time": "2020-11-13T17:39:15.276Z",
    "type": "execution"
   },
   {
    "code": "def words_in_texts(words, texts):\n    '''\n    Args:\n        words (list): words to find\n        texts (Series): strings to search in\n    \n    Returns:\n        NumPy array of 0s and 1s with shape (n, p) where n is the\n        number of texts and p is the number of words.\n    '''\n    indicator_array = 1 * np.array([texts.str.contains(word) for word in words]).T # SOLUTION\n    return indicator_array",
    "id": "81aa3a295d054268acb33351ad567192",
    "idx": 26,
    "time": "2020-11-13T17:39:15.279Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nnp.allclose(words_in_texts(['hello', 'bye', 'world'], \n                           pd.Series(['hello', 'hello worldhello'])),\n            np.array([[1, 0, 0], \n                      [1, 0, 1]]))",
    "id": "690ffc0aa5e446949b45a8b4ecd76f85",
    "idx": 27,
    "time": "2020-11-13T17:39:15.282Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nnp.allclose(words_in_texts(['a', 'b', 'c', 'd', 'e', 'f', 'g'], \n                           pd.Series(['a b c d ef g', 'a', 'b', 'c', 'd e f g', 'h', 'a h'])),\n            np.array([[1,1,1,1,1,1,1], \n                      [1,0,0,0,0,0,0],\n                      [0,1,0,0,0,0,0],\n                      [0,0,1,0,0,0,0],\n                      [0,0,0,1,1,1,1],\n                      [0,0,0,0,0,0,0],\n                      [1,0,0,0,0,0,0]]))",
    "id": "c3fba6032da048efaf5bf9b349cfb4f8",
    "idx": 28,
    "time": "2020-11-13T17:39:15.284Z",
    "type": "execution"
   },
   {
    "code": "from IPython.display import display, Markdown\ndf = pd.DataFrame({\n    'word_1': [1, 0, 1, 0],\n    'word_2': [0, 1, 0, 1],\n    'type': ['spam', 'ham', 'ham', 'ham']\n})\ndisplay(Markdown(\"> Our Original DataFrame has a `type` column and some columns corresponding to words. You can think of each row as a sentence, and the value of 1 or 0 indicates the number of occurences of the word in this sentence.\"))\ndisplay(df);\ndisplay(Markdown(\"> `melt` will turn columns into entries in a variable column. Notice how `word_1` and `word_2` become entries in `variable`; their values are stored in the value column.\"))\ndisplay(df.melt(\"type\"))",
    "id": "c248bceea6bd440c870eb7f942fcf77e",
    "idx": 31,
    "time": "2020-11-13T17:39:15.289Z",
    "type": "execution"
   },
   {
    "code": "train=train.reset_index(drop=True) # We must do this in order to preserve the ordering of emails to labels for words_in_texts\n\n# BEGIN SOLUTION\nsome_words = ['opportunity', 'bank', 'receive', 'dear', 'best', 'deal']\nPhi_train = words_in_texts(some_words, train['email'])\n\ndf = pd.DataFrame(data = Phi_train, columns = some_words)\ndf['label'] = train['spam']\n\nplt.figure(figsize=(12,8))\nsns.barplot(x = \"variable\", \n            y = \"value\", \n            hue = \"label\", \n            data = (df\n                    .replace({'label': \n                                {0 : 'Ham', \n                                 1 : 'Spam'}})\n                    .melt('label')\n                    .groupby(['label', 'variable'])\n                    .mean()\n                    .reset_index()))\n\nplt.xlabel('Words')\nplt.ylabel('Proportion of Emails')\nplt.legend(title = \"\")\nplt.title(\"Frequency of Words in Spam/Ham Emails\")\nplt.tight_layout()\nplt.show()\n# END SOLUTION",
    "id": "3da1bb8528f1420d8e4b01106abb0d1c",
    "idx": 33,
    "time": "2020-11-13T17:39:15.291Z",
    "type": "execution"
   },
   {
    "code": "# BEGIN SOLUTION\ntmp = train.copy()\ntmp['length'] = tmp['email'].str.len()\nplt.figure(figsize=(10, 6))\nsns.distplot(tmp.loc[tmp['spam'] == 0, 'length'],hist=False, label='Ham')\nsns.distplot(tmp.loc[tmp['spam'] == 1, 'length'],hist=False, label='Spam')\nplt.title('Distribution of Email Body Length')\nplt.xlabel('Length of email body')\nplt.ylabel('Distribution')\nplt.xlim((0,50000))\nplt.tight_layout()\nplt.legend();\n# END SOLUTION",
    "id": "ccae3bbd17a7472584923d1fd81f0c4b",
    "idx": 36,
    "time": "2020-11-13T17:39:15.295Z",
    "type": "execution"
   },
   {
    "code": "some_words = ['drug', 'bank', 'prescription', 'memo', 'private']\n\nX_train = words_in_texts(some_words, train['email']) # SOLUTION\nY_train = np.array(train['spam']) # SOLUTION\n\nX_train[:5], Y_train[:5]",
    "id": "4228df6326c449d782e8471fb2f787fe",
    "idx": 39,
    "time": "2020-11-13T17:39:15.297Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nX_train.shape == (7513, 5)",
    "id": "16be536580a04248971c0fcb94bb3f0e",
    "idx": 40,
    "time": "2020-11-13T17:39:15.299Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nnp.array_equal(np.unique(X_train), np.array([0, 1])) # X matrix should consist of only 0 or 1",
    "id": "dc0cb916fcc5470885a644448fa6684c",
    "idx": 41,
    "time": "2020-11-13T17:39:15.301Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nnp.array_equal(np.unique(Y_train), np.array([0, 1])) # y vector should consist of only 0 or 1",
    "id": "7b8699ffc3414488a9d374ae3a762231",
    "idx": 42,
    "time": "2020-11-13T17:39:15.303Z",
    "type": "execution"
   },
   {
    "code": "from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(solver = 'lbfgs') # SOLUTION\nmodel.fit(X_train, Y_train) # SOLUTION\n\ntraining_accuracy = model.score(X_train, Y_train) # SOLUTION\nprint(\"Training Accuracy: \", training_accuracy)",
    "id": "6a1a859678394a819af601759e1b4c0d",
    "idx": 44,
    "time": "2020-11-13T17:39:15.305Z",
    "type": "execution"
   },
   {
    "code": "# TEST\ntraining_accuracy > 0.72",
    "id": "45afd18c3a604fd8825017255e72891e",
    "idx": 45,
    "time": "2020-11-13T17:39:15.307Z",
    "type": "execution"
   },
   {
    "code": "zero_predictor_fp = 0 # SOLUTION\nzero_predictor_fn = sum(Y_train == 1) # SOLUTION\nzero_predictor_fp, zero_predictor_fn",
    "id": "30f5741e6f4742119d46340efe2e6a52",
    "idx": 49,
    "time": "2020-11-13T17:39:15.310Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nzero_predictor_fp >= 0",
    "id": "23064baca6924c4c9f2d7f478494c007",
    "idx": 50,
    "time": "2020-11-13T17:39:15.312Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nzero_predictor_fn >= 0",
    "id": "3475b02fb61a47839320a7c9edd5d518",
    "idx": 51,
    "time": "2020-11-13T17:39:15.314Z",
    "type": "execution"
   },
   {
    "code": "# HIDDEN TEST\nnp.isclose(zero_predictor_fp, 0)",
    "id": "feef0e913db14b6b8101c4489462b487",
    "idx": 52,
    "time": "2020-11-13T17:39:15.316Z",
    "type": "execution"
   },
   {
    "code": "# HIDDEN TEST\nzero_predictor_fn == 1918",
    "id": "8dde97c384214925a9c7f42f58f87aaf",
    "idx": 53,
    "time": "2020-11-13T17:39:15.318Z",
    "type": "execution"
   },
   {
    "code": "zero_predictor_acc = np.mean(Y_train == 0) # SOLUTION\nzero_predictor_recall = 0 # SOLUTION\nzero_predictor_acc, zero_predictor_recall",
    "id": "933ef39465ec409e96b3a53f6203baab",
    "idx": 55,
    "time": "2020-11-13T17:39:15.321Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nzero_predictor_acc >= 0",
    "id": "2499a1d760f1441299b055adc8736e68",
    "idx": 56,
    "time": "2020-11-13T17:39:15.322Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nzero_predictor_recall >= 0",
    "id": "2de51ed2ec9f4e11be1000fb204774a3",
    "idx": 57,
    "time": "2020-11-13T17:39:15.324Z",
    "type": "execution"
   },
   {
    "code": "# HIDDEN TEST\nnp.isclose(zero_predictor_acc, 0.7447091707706642)",
    "id": "ff5d5782535a42019b7eb5a8ec2753dc",
    "idx": 58,
    "time": "2020-11-13T17:39:15.325Z",
    "type": "execution"
   },
   {
    "code": "# HIDDEN TEST\nnp.isclose(zero_predictor_recall, 0)",
    "id": "f0850a5e5e024d2d9f9bcafe925e1901",
    "idx": 59,
    "time": "2020-11-13T17:39:15.327Z",
    "type": "execution"
   },
   {
    "code": "# BEGIN SOLUTION NO PROMPT\nY_train_hat = model.predict(X_train)\n\nTP = sum((Y_train_hat == Y_train) & (Y_train_hat == 1))\nTN = sum((Y_train_hat == Y_train) & (Y_train_hat == 0))\nFP = sum((Y_train_hat != Y_train) & (Y_train_hat == 1))\nFN = sum((Y_train_hat != Y_train) & (Y_train_hat == 0))\n# END SOLUTION\nlogistic_predictor_precision = TP / (TP + FP) # SOLUTION\nlogistic_predictor_recall = TP / (TP + FN) # SOLUTION\nlogistic_predictor_far = FP / (FP + TN) # SOLUTION",
    "id": "aa340a7e7443411fb115580a71b83b27",
    "idx": 63,
    "time": "2020-11-13T17:39:15.330Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nlogistic_predictor_precision >= 0",
    "id": "2be7e72466a649f1822d8f8458cf3921",
    "idx": 64,
    "time": "2020-11-13T17:39:15.332Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nlogistic_predictor_recall >= 0",
    "id": "70a7e749219e434092993513497dc589",
    "idx": 65,
    "time": "2020-11-13T17:39:15.335Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nlogistic_predictor_far >= 0",
    "id": "c555cae2efe24a11960d966fee18793b",
    "idx": 66,
    "time": "2020-11-13T17:39:15.335Z",
    "type": "execution"
   },
   {
    "code": "# HIDDEN TEST\nnp.isclose(logistic_predictor_precision, 0.6422287390029325)",
    "id": "84339b96007044c18bb3370123e713d5",
    "idx": 67,
    "time": "2020-11-13T17:39:15.338Z",
    "type": "execution"
   },
   {
    "code": "# HIDDEN TEST\nnp.isclose(logistic_predictor_recall, 0.11418143899895725)",
    "id": "e9752dbea367428782b9be92a30dfd54",
    "idx": 68,
    "time": "2020-11-13T17:39:15.340Z",
    "type": "execution"
   },
   {
    "code": "# HIDDEN TEST\nnp.isclose(logistic_predictor_far, 0.021805183199285077)",
    "id": "b7177c97cf0a4893a65e2229d24bddde",
    "idx": 69,
    "time": "2020-11-13T17:39:15.341Z",
    "type": "execution"
   },
   {
    "code": "# Write your description (2-3 sentences) as a comment here:\n# \n#\n#\n\n# Write the code to generate your visualization here:\n# BEGIN SOLUTION\nplt.plot([1, 3, 5]) # This is a dummy plot, not a real example of a solution\n# END SOLUTION",
    "id": "8c0c0ab98ec64e0f843fdaaa8287e1b3",
    "idx": 79,
    "time": "2020-11-13T17:39:15.347Z",
    "type": "execution"
   },
   {
    "code": "from sklearn.metrics import roc_curve\n\n# Note that you'll want to use the .predict_proba(...) method for your classifier\n# instead of .predict(...) so you get probabilities, not classes\n\n# BEGIN SOLUTION\nstaff_words = ['body', 'click', 'please', 'base64', '2002', 'html', 'subscribed',\n               'wrote', 'mortgage', 'align3dcenterfont', 'dear', 'br', 'width10img',\n               'divfont', 'im', 'receive', 'list', 'tags', 'web', 'base64', 'click',\n               'body', 'please', 'money', 'offer', 'receive', 'contact', 'free',\n               'tr', 'removed', 'remove', 'html', 'font', 'form',\n               'credit', 'business', 'div']\n\nX_train_2 = words_in_texts(staff_words, train['email'])\n\nstaff_model = LogisticRegression(solver = 'lbfgs')\nstaff_model.fit(X_train_2, Y_train)\n\nprint('accuracy: ', staff_model.score(X_train_2, Y_train))\n\nY_predict = staff_model.predict_proba(X_train_2)[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_train, Y_predict)\nwith sns.axes_style(\"white\"):\n    plt.plot(fpr, tpr)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.show()\n# END SOLUTION",
    "id": "8059141c245f48d1843bcdba92a55cd9",
    "idx": 81,
    "time": "2020-11-13T17:39:15.349Z",
    "type": "execution"
   },
   {
    "code": "test_predictions = staff_model.predict(words_in_texts(staff_words, test['email'])) # SOLUTION",
    "id": "b3b715b7bbe440a29afc031436448f68",
    "idx": 83,
    "time": "2020-11-13T17:39:15.351Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nisinstance(test_predictions, np.ndarray) # must be ndarray of predictions",
    "id": "4b8c3b3000f8467d8e92f63713c5a948",
    "idx": 84,
    "time": "2020-11-13T17:39:15.353Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nnp.array_equal(np.unique(test_predictions), np.array([0, 1])) # must be binary labels (0 or 1) and not probabilities",
    "id": "e49cb9eca4884f10bd54709c678f11c4",
    "idx": 85,
    "time": "2020-11-13T17:39:15.355Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nlen(test_predictions) == 1000 # must be the right number of predictions",
    "id": "558f42cfe8c64125afce510c3b7d9647",
    "idx": 86,
    "time": "2020-11-13T17:39:15.357Z",
    "type": "execution"
   },
   {
    "code": "from datetime import datetime\n\n# Assuming that your predictions on the test set are stored in a 1-dimensional array called\n# test_predictions. Feel free to modify this cell as long you create a CSV in the right format.\n\n# Construct and save the submission:\nsubmission_df = pd.DataFrame({\n    \"Id\": test['id'], \n    \"Class\": test_predictions,\n}, columns=['Id', 'Class'])\ntimestamp = datetime.isoformat(datetime.now()).split(\".\")[0]\nsubmission_df.to_csv(\"submission_{}.csv\".format(timestamp), index=False)\n\nprint('Created a CSV file: {}.'.format(\"submission_{}.csv\".format(timestamp)))\nprint('You may now upload this CSV file to Gradescope for scoring.')",
    "id": "951a8f66c9384b52b03fd07762ba6645",
    "idx": 88,
    "time": "2020-11-13T17:39:15.359Z",
    "type": "execution"
   },
   {
    "id": "bf4059a762a047a6a8e4179dc2c11815",
    "time": "2020-11-13T17:39:15.381Z",
    "type": "completion"
   },
   {
    "id": "679de399389b410fbd1bd229389869ba",
    "time": "2020-11-13T17:39:16.314Z",
    "type": "completion"
   },
   {
    "id": "668f75f782c041508f9e0c8f82311ad1",
    "time": "2020-11-13T17:39:17.398Z",
    "type": "completion"
   },
   {
    "id": "68e38b511a2c4f5480d4eda2b0082a11",
    "time": "2020-11-13T17:39:17.402Z",
    "type": "completion"
   },
   {
    "id": "7f4b00c62776443e817cd5a255aa14b9",
    "time": "2020-11-13T17:39:17.451Z",
    "type": "completion"
   },
   {
    "id": "bfb55fdfdfb1456785bb78790aad426b",
    "time": "2020-11-13T17:39:17.502Z",
    "type": "completion"
   },
   {
    "id": "6345c5019a384a618017acbb080996d9",
    "time": "2020-11-13T17:39:17.557Z",
    "type": "completion"
   },
   {
    "id": "ee844f20ed0a41c4b81ff495fe482bdf",
    "time": "2020-11-13T17:39:17.641Z",
    "type": "completion"
   },
   {
    "id": "13a350f12fac4931be17235a1daf0a93",
    "time": "2020-11-13T17:39:17.687Z",
    "type": "completion"
   },
   {
    "id": "a90683e4a73346878a06d732d7101c87",
    "time": "2020-11-13T17:39:17.724Z",
    "type": "completion"
   },
   {
    "id": "7523257e508447369ca710ce27eba7b1",
    "time": "2020-11-13T17:39:17.810Z",
    "type": "completion"
   },
   {
    "id": "81aa3a295d054268acb33351ad567192",
    "time": "2020-11-13T17:39:17.822Z",
    "type": "completion"
   },
   {
    "id": "690ffc0aa5e446949b45a8b4ecd76f85",
    "time": "2020-11-13T17:39:17.854Z",
    "type": "completion"
   },
   {
    "id": "c3fba6032da048efaf5bf9b349cfb4f8",
    "time": "2020-11-13T17:39:17.895Z",
    "type": "completion"
   },
   {
    "id": "c248bceea6bd440c870eb7f942fcf77e",
    "time": "2020-11-13T17:39:17.923Z",
    "type": "completion"
   },
   {
    "id": "3da1bb8528f1420d8e4b01106abb0d1c",
    "time": "2020-11-13T17:39:18.623Z",
    "type": "completion"
   },
   {
    "id": "ccae3bbd17a7472584923d1fd81f0c4b",
    "time": "2020-11-13T17:39:19.061Z",
    "type": "completion"
   },
   {
    "id": "4228df6326c449d782e8471fb2f787fe",
    "time": "2020-11-13T17:39:19.120Z",
    "type": "completion"
   },
   {
    "id": "16be536580a04248971c0fcb94bb3f0e",
    "time": "2020-11-13T17:39:19.127Z",
    "type": "completion"
   },
   {
    "id": "dc0cb916fcc5470885a644448fa6684c",
    "time": "2020-11-13T17:39:19.185Z",
    "type": "completion"
   },
   {
    "id": "7b8699ffc3414488a9d374ae3a762231",
    "time": "2020-11-13T17:39:19.247Z",
    "type": "completion"
   },
   {
    "id": "6a1a859678394a819af601759e1b4c0d",
    "time": "2020-11-13T17:39:19.371Z",
    "type": "completion"
   },
   {
    "id": "45afd18c3a604fd8825017255e72891e",
    "time": "2020-11-13T17:39:19.383Z",
    "type": "completion"
   },
   {
    "id": "30f5741e6f4742119d46340efe2e6a52",
    "time": "2020-11-13T17:39:19.485Z",
    "type": "completion"
   },
   {
    "id": "23064baca6924c4c9f2d7f478494c007",
    "time": "2020-11-13T17:39:19.536Z",
    "type": "completion"
   },
   {
    "id": "3475b02fb61a47839320a7c9edd5d518",
    "time": "2020-11-13T17:39:19.588Z",
    "type": "completion"
   },
   {
    "id": "feef0e913db14b6b8101c4489462b487",
    "time": "2020-11-13T17:39:19.642Z",
    "type": "completion"
   },
   {
    "id": "8dde97c384214925a9c7f42f58f87aaf",
    "time": "2020-11-13T17:39:19.709Z",
    "type": "completion"
   },
   {
    "id": "933ef39465ec409e96b3a53f6203baab",
    "time": "2020-11-13T17:39:19.769Z",
    "type": "completion"
   },
   {
    "id": "2499a1d760f1441299b055adc8736e68",
    "time": "2020-11-13T17:39:19.867Z",
    "type": "completion"
   },
   {
    "id": "2de51ed2ec9f4e11be1000fb204774a3",
    "time": "2020-11-13T17:39:20.000Z",
    "type": "completion"
   },
   {
    "id": "ff5d5782535a42019b7eb5a8ec2753dc",
    "time": "2020-11-13T17:39:20.086Z",
    "type": "completion"
   },
   {
    "id": "f0850a5e5e024d2d9f9bcafe925e1901",
    "time": "2020-11-13T17:39:20.167Z",
    "type": "completion"
   },
   {
    "id": "aa340a7e7443411fb115580a71b83b27",
    "time": "2020-11-13T17:39:20.390Z",
    "type": "completion"
   },
   {
    "id": "2be7e72466a649f1822d8f8458cf3921",
    "time": "2020-11-13T17:39:20.396Z",
    "type": "completion"
   },
   {
    "id": "70a7e749219e434092993513497dc589",
    "time": "2020-11-13T17:39:20.469Z",
    "type": "completion"
   },
   {
    "id": "c555cae2efe24a11960d966fee18793b",
    "time": "2020-11-13T17:39:20.553Z",
    "type": "completion"
   },
   {
    "id": "84339b96007044c18bb3370123e713d5",
    "time": "2020-11-13T17:39:20.650Z",
    "type": "completion"
   },
   {
    "id": "e9752dbea367428782b9be92a30dfd54",
    "time": "2020-11-13T17:39:20.717Z",
    "type": "completion"
   },
   {
    "id": "b7177c97cf0a4893a65e2229d24bddde",
    "time": "2020-11-13T17:39:20.772Z",
    "type": "completion"
   },
   {
    "id": "8c0c0ab98ec64e0f843fdaaa8287e1b3",
    "time": "2020-11-13T17:39:21.057Z",
    "type": "completion"
   },
   {
    "id": "8059141c245f48d1843bcdba92a55cd9",
    "time": "2020-11-13T17:39:22.358Z",
    "type": "completion"
   },
   {
    "id": "b3b715b7bbe440a29afc031436448f68",
    "time": "2020-11-13T17:39:22.422Z",
    "type": "completion"
   },
   {
    "id": "4b8c3b3000f8467d8e92f63713c5a948",
    "time": "2020-11-13T17:39:22.456Z",
    "type": "completion"
   },
   {
    "id": "e49cb9eca4884f10bd54709c678f11c4",
    "time": "2020-11-13T17:39:22.531Z",
    "type": "completion"
   },
   {
    "id": "558f42cfe8c64125afce510c3b7d9647",
    "time": "2020-11-13T17:39:22.633Z",
    "type": "completion"
   },
   {
    "id": "951a8f66c9384b52b03fd07762ba6645",
    "time": "2020-11-13T17:39:22.769Z",
    "type": "completion"
   },
   {
    "code": "sns.distplot(tmp[tmp['spam']==0].length)",
    "id": "7d23973917c7446e89e5254e568f0870",
    "idx": 37,
    "time": "2020-11-13T17:47:19.110Z",
    "type": "execution"
   },
   {
    "id": "7d23973917c7446e89e5254e568f0870",
    "time": "2020-11-13T17:47:19.599Z",
    "type": "completion"
   },
   {
    "code": "sns.distplot(tmp[tmp['spam']==1].length)",
    "id": "3fa95f246e09494289e5530c5b1afc30",
    "idx": 37,
    "time": "2020-11-13T17:47:27.053Z",
    "type": "execution"
   },
   {
    "id": "3fa95f246e09494289e5530c5b1afc30",
    "time": "2020-11-13T17:47:27.538Z",
    "type": "completion"
   },
   {
    "code": "sns.distplot(tmp[tmp['spam']==1].length)\nplt.xlim(0, 50000)",
    "id": "3fa95f246e09494289e5530c5b1afc30",
    "idx": 37,
    "time": "2020-11-13T17:47:41.149Z",
    "type": "execution"
   },
   {
    "id": "3fa95f246e09494289e5530c5b1afc30",
    "time": "2020-11-13T17:47:41.623Z",
    "type": "completion"
   },
   {
    "code": "sns.distplot(tmp[tmp['spam']==0].length)\nplt.xlim(0, 50000)",
    "id": "7d23973917c7446e89e5254e568f0870",
    "idx": 38,
    "time": "2020-11-13T17:47:45.930Z",
    "type": "execution"
   },
   {
    "id": "7d23973917c7446e89e5254e568f0870",
    "time": "2020-11-13T17:47:46.515Z",
    "type": "completion"
   },
   {
    "code": "# BEGIN SOLUTION\ntmp = train.copy()\ntmp['length'] = tmp['email'].str.len()\nplt.figure(figsize=(10, 6))\nsns.distplot(tmp.loc[tmp['spam'] == 0, 'length'],hist=False, label='Ham')\nsns.distplot(tmp.loc[tmp['spam'] == 1, 'length'],hist=False, label='Spam')\nplt.title('Distribution of Email Body Length')\nplt.xlabel('Length of email body')\nplt.ylabel('Distribution')\nplt.xlim((0,50000))\nplt.tight_layout()\nplt.legend();\n# END SOLUTION\nplt.savefig('training_conditional_densities.png')",
    "id": "ccae3bbd17a7472584923d1fd81f0c4b",
    "idx": 36,
    "time": "2020-11-13T17:48:23.568Z",
    "type": "execution"
   },
   {
    "id": "ccae3bbd17a7472584923d1fd81f0c4b",
    "time": "2020-11-13T17:48:24.403Z",
    "type": "completion"
   },
   {
    "code": "# Run this cell to suppress all FutureWarnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)",
    "id": "bf4059a762a047a6a8e4179dc2c11815",
    "idx": 5,
    "time": "2020-11-13T17:50:40.136Z",
    "type": "execution"
   },
   {
    "id": "bf4059a762a047a6a8e4179dc2c11815",
    "time": "2020-11-13T17:50:40.313Z",
    "type": "completion"
   },
   {
    "code": "import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set(style = \"whitegrid\", \n        color_codes = True,\n        font_scale = 1.5)",
    "id": "679de399389b410fbd1bd229389869ba",
    "idx": 8,
    "time": "2020-11-13T17:50:40.821Z",
    "type": "execution"
   },
   {
    "id": "679de399389b410fbd1bd229389869ba",
    "time": "2020-11-13T17:50:40.897Z",
    "type": "completion"
   },
   {
    "code": "from utils import fetch_and_cache_gdrive\nfetch_and_cache_gdrive('1SCASpLZFKCp2zek-toR3xeKX3DZnBSyp', 'train.csv')\nfetch_and_cache_gdrive('1ZDFo9OTF96B5GP2Nzn8P8-AL7CTQXmC0', 'test.csv')\n\noriginal_training_data = pd.read_csv('data/train.csv')\ntest = pd.read_csv('data/test.csv')\n\n# Convert the emails to lower case as a first step to processing the text\noriginal_training_data['email'] = original_training_data['email'].str.lower()\ntest['email'] = test['email'].str.lower()\n\noriginal_training_data.head()",
    "id": "668f75f782c041508f9e0c8f82311ad1",
    "idx": 10,
    "time": "2020-11-13T17:50:41.062Z",
    "type": "execution"
   },
   {
    "code": "# BEGIN SOLUTION\nprint('Before imputation:')\nprint(original_training_data.isnull().sum())\noriginal_training_data = original_training_data.fillna('')\nprint('------------')\nprint('After imputation:')\nprint(original_training_data.isnull().sum())\n# END SOLUTION",
    "id": "68e38b511a2c4f5480d4eda2b0082a11",
    "idx": 12,
    "time": "2020-11-13T17:50:41.287Z",
    "type": "execution"
   },
   {
    "code": "# TEST\noriginal_training_data.isnull().sum().sum() == 0",
    "id": "7f4b00c62776443e817cd5a255aa14b9",
    "idx": 13,
    "time": "2020-11-13T17:50:41.411Z",
    "type": "execution"
   },
   {
    "id": "668f75f782c041508f9e0c8f82311ad1",
    "time": "2020-11-13T17:50:41.553Z",
    "type": "completion"
   },
   {
    "id": "68e38b511a2c4f5480d4eda2b0082a11",
    "time": "2020-11-13T17:50:41.562Z",
    "type": "completion"
   },
   {
    "id": "7f4b00c62776443e817cd5a255aa14b9",
    "time": "2020-11-13T17:50:41.606Z",
    "type": "completion"
   },
   {
    "code": "first_ham = original_training_data.loc[original_training_data['spam'] == 0, 'email'].iloc[0] # SOLUTION\nfirst_spam = original_training_data.loc[original_training_data['spam'] == 1, 'email'].iloc[0] # SOLUTION\nprint(first_ham)\nprint(first_spam)",
    "id": "bfb55fdfdfb1456785bb78790aad426b",
    "idx": 15,
    "time": "2020-11-13T17:50:42.451Z",
    "type": "execution"
   },
   {
    "id": "bfb55fdfdfb1456785bb78790aad426b",
    "time": "2020-11-13T17:50:42.530Z",
    "type": "completion"
   },
   {
    "code": "# TEST\nlen(first_ham) > 0 and first_ham[:0] == ''",
    "id": "6345c5019a384a618017acbb080996d9",
    "idx": 16,
    "time": "2020-11-13T17:50:42.586Z",
    "type": "execution"
   },
   {
    "id": "6345c5019a384a618017acbb080996d9",
    "time": "2020-11-13T17:50:42.656Z",
    "type": "completion"
   },
   {
    "code": "# TEST\nlen(first_spam) > 0 and first_spam[:0] == ''",
    "id": "ee844f20ed0a41c4b81ff495fe482bdf",
    "idx": 17,
    "time": "2020-11-13T17:50:42.774Z",
    "type": "execution"
   },
   {
    "id": "ee844f20ed0a41c4b81ff495fe482bdf",
    "time": "2020-11-13T17:50:42.839Z",
    "type": "completion"
   },
   {
    "code": "# HIDDEN TEST\noriginal_training_data.loc[original_training_data['spam'] == 0, 'email'].iloc[0] in first_ham",
    "id": "13a350f12fac4931be17235a1daf0a93",
    "idx": 18,
    "time": "2020-11-13T17:50:42.995Z",
    "type": "execution"
   },
   {
    "id": "13a350f12fac4931be17235a1daf0a93",
    "time": "2020-11-13T17:50:43.069Z",
    "type": "completion"
   },
   {
    "code": "# HIDDEN TEST\noriginal_training_data.loc[original_training_data['spam'] == 1, 'email'].iloc[0] in first_spam",
    "id": "a90683e4a73346878a06d732d7101c87",
    "idx": 19,
    "time": "2020-11-13T17:50:43.103Z",
    "type": "execution"
   },
   {
    "id": "a90683e4a73346878a06d732d7101c87",
    "time": "2020-11-13T17:50:43.180Z",
    "type": "completion"
   },
   {
    "code": "# This creates a 90/10 train-validation split on our labeled data\n\nfrom sklearn.model_selection import train_test_split\n\ntrain, val = train_test_split(original_training_data, test_size=0.1, random_state=42)",
    "id": "7523257e508447369ca710ce27eba7b1",
    "idx": 23,
    "time": "2020-11-13T17:50:43.719Z",
    "type": "execution"
   },
   {
    "id": "7523257e508447369ca710ce27eba7b1",
    "time": "2020-11-13T17:50:43.793Z",
    "type": "completion"
   },
   {
    "code": "def words_in_texts(words, texts):\n    '''\n    Args:\n        words (list): words to find\n        texts (Series): strings to search in\n    \n    Returns:\n        NumPy array of 0s and 1s with shape (n, p) where n is the\n        number of texts and p is the number of words.\n    '''\n    indicator_array = 1 * np.array([texts.str.contains(word) for word in words]).T # SOLUTION\n    return indicator_array",
    "id": "81aa3a295d054268acb33351ad567192",
    "idx": 26,
    "time": "2020-11-13T17:50:44.197Z",
    "type": "execution"
   },
   {
    "id": "81aa3a295d054268acb33351ad567192",
    "time": "2020-11-13T17:50:44.266Z",
    "type": "completion"
   },
   {
    "code": "# TEST\nnp.allclose(words_in_texts(['hello', 'bye', 'world'], \n                           pd.Series(['hello', 'hello worldhello'])),\n            np.array([[1, 0, 0], \n                      [1, 0, 1]]))",
    "id": "690ffc0aa5e446949b45a8b4ecd76f85",
    "idx": 27,
    "time": "2020-11-13T17:50:44.320Z",
    "type": "execution"
   },
   {
    "id": "690ffc0aa5e446949b45a8b4ecd76f85",
    "time": "2020-11-13T17:50:44.392Z",
    "type": "completion"
   },
   {
    "code": "# TEST\nnp.allclose(words_in_texts(['a', 'b', 'c', 'd', 'e', 'f', 'g'], \n                           pd.Series(['a b c d ef g', 'a', 'b', 'c', 'd e f g', 'h', 'a h'])),\n            np.array([[1,1,1,1,1,1,1], \n                      [1,0,0,0,0,0,0],\n                      [0,1,0,0,0,0,0],\n                      [0,0,1,0,0,0,0],\n                      [0,0,0,1,1,1,1],\n                      [0,0,0,0,0,0,0],\n                      [1,0,0,0,0,0,0]]))",
    "id": "c3fba6032da048efaf5bf9b349cfb4f8",
    "idx": 28,
    "time": "2020-11-13T17:50:44.456Z",
    "type": "execution"
   },
   {
    "id": "c3fba6032da048efaf5bf9b349cfb4f8",
    "time": "2020-11-13T17:50:44.532Z",
    "type": "completion"
   },
   {
    "code": "from IPython.display import display, Markdown\ndf = pd.DataFrame({\n    'word_1': [1, 0, 1, 0],\n    'word_2': [0, 1, 0, 1],\n    'type': ['spam', 'ham', 'ham', 'ham']\n})\ndisplay(Markdown(\"> Our Original DataFrame has a `type` column and some columns corresponding to words. You can think of each row as a sentence, and the value of 1 or 0 indicates the number of occurences of the word in this sentence.\"))\ndisplay(df);\ndisplay(Markdown(\"> `melt` will turn columns into entries in a variable column. Notice how `word_1` and `word_2` become entries in `variable`; their values are stored in the value column.\"))\ndisplay(df.melt(\"type\"))",
    "id": "c248bceea6bd440c870eb7f942fcf77e",
    "idx": 31,
    "time": "2020-11-13T17:50:44.843Z",
    "type": "execution"
   },
   {
    "id": "c248bceea6bd440c870eb7f942fcf77e",
    "time": "2020-11-13T17:50:44.938Z",
    "type": "completion"
   },
   {
    "code": "train=train.reset_index(drop=True) # We must do this in order to preserve the ordering of emails to labels for words_in_texts\n\n# BEGIN SOLUTION\nsome_words = ['opportunity', 'bank', 'receive', 'dear', 'best', 'deal']\nPhi_train = words_in_texts(some_words, train['email'])\n\ndf = pd.DataFrame(data = Phi_train, columns = some_words)\ndf['label'] = train['spam']\n\nplt.figure(figsize=(12,8))\nsns.barplot(x = \"variable\", \n            y = \"value\", \n            hue = \"label\", \n            data = (df\n                    .replace({'label': \n                                {0 : 'Ham', \n                                 1 : 'Spam'}})\n                    .melt('label')\n                    .groupby(['label', 'variable'])\n                    .mean()\n                    .reset_index()))\n\nplt.xlabel('Words')\nplt.ylabel('Proportion of Emails')\nplt.legend(title = \"\")\nplt.title(\"Frequency of Words in Spam/Ham Emails\")\nplt.tight_layout()\nplt.show()\n# END SOLUTION",
    "id": "3da1bb8528f1420d8e4b01106abb0d1c",
    "idx": 33,
    "time": "2020-11-13T17:50:45.085Z",
    "type": "execution"
   },
   {
    "code": "# BEGIN SOLUTION\ntmp = train.copy()\ntmp['length'] = tmp['email'].str.len()\nplt.figure(figsize=(10, 6))\nsns.distplot(tmp.loc[tmp['spam'] == 0, 'length'],hist=False, label='Ham')\nsns.distplot(tmp.loc[tmp['spam'] == 1, 'length'],hist=False, label='Spam')\nplt.title('Distribution of Email Body Length')\nplt.xlabel('Length of email body')\nplt.ylabel('Distribution')\nplt.xlim((0,50000))\nplt.tight_layout()\nplt.legend();\n# END SOLUTION\nplt.savefig('training_conditional_densities.png')",
    "id": "ccae3bbd17a7472584923d1fd81f0c4b",
    "idx": 36,
    "time": "2020-11-13T17:50:45.529Z",
    "type": "execution"
   },
   {
    "id": "3da1bb8528f1420d8e4b01106abb0d1c",
    "time": "2020-11-13T17:50:45.785Z",
    "type": "completion"
   },
   {
    "id": "ccae3bbd17a7472584923d1fd81f0c4b",
    "time": "2020-11-13T17:50:46.397Z",
    "type": "completion"
   },
   {
    "code": "# Run this cell to suppress all FutureWarnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)",
    "id": "5717cb1fad9e4423882ebb7402a960d0",
    "idx": 5,
    "time": "2020-11-13T17:52:54.134Z",
    "type": "execution"
   },
   {
    "id": "5717cb1fad9e4423882ebb7402a960d0",
    "time": "2020-11-13T17:52:54.207Z",
    "type": "completion"
   },
   {
    "code": "import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set(style = \"whitegrid\", \n        color_codes = True,\n        font_scale = 1.5)",
    "id": "922a4e60525f4eafb7a67d51618380a8",
    "idx": 8,
    "time": "2020-11-13T17:52:54.451Z",
    "type": "execution"
   },
   {
    "id": "922a4e60525f4eafb7a67d51618380a8",
    "time": "2020-11-13T17:52:54.611Z",
    "type": "completion"
   },
   {
    "code": "from utils import fetch_and_cache_gdrive\nfetch_and_cache_gdrive('1SCASpLZFKCp2zek-toR3xeKX3DZnBSyp', 'train.csv')\nfetch_and_cache_gdrive('1ZDFo9OTF96B5GP2Nzn8P8-AL7CTQXmC0', 'test.csv')\n\noriginal_training_data = pd.read_csv('data/train.csv')\ntest = pd.read_csv('data/test.csv')\n\n# Convert the emails to lower case as a first step to processing the text\noriginal_training_data['email'] = original_training_data['email'].str.lower()\ntest['email'] = test['email'].str.lower()\n\noriginal_training_data.head()",
    "id": "5b789cd4cf994c438c36c83a11f0883e",
    "idx": 10,
    "time": "2020-11-13T17:52:54.797Z",
    "type": "execution"
   },
   {
    "code": "# BEGIN SOLUTION\nprint('Before imputation:')\nprint(original_training_data.isnull().sum())\noriginal_training_data = original_training_data.fillna('')\nprint('------------')\nprint('After imputation:')\nprint(original_training_data.isnull().sum())\n# END SOLUTION",
    "id": "8cc51a7378104317923b12eb8010193c",
    "idx": 12,
    "time": "2020-11-13T17:52:55.069Z",
    "type": "execution"
   },
   {
    "code": "# TEST\noriginal_training_data.isnull().sum().sum() == 0",
    "id": "fd0fc36dcb3e465fa5de88357798251b",
    "idx": 13,
    "time": "2020-11-13T17:52:55.204Z",
    "type": "execution"
   },
   {
    "id": "5b789cd4cf994c438c36c83a11f0883e",
    "time": "2020-11-13T17:52:55.275Z",
    "type": "completion"
   },
   {
    "id": "8cc51a7378104317923b12eb8010193c",
    "time": "2020-11-13T17:52:55.285Z",
    "type": "completion"
   },
   {
    "id": "fd0fc36dcb3e465fa5de88357798251b",
    "time": "2020-11-13T17:52:55.362Z",
    "type": "completion"
   },
   {
    "code": "first_ham = original_training_data.loc[original_training_data['spam'] == 0, 'email'].iloc[0] # SOLUTION\nfirst_spam = original_training_data.loc[original_training_data['spam'] == 1, 'email'].iloc[0] # SOLUTION\nprint(first_ham)\nprint(first_spam)",
    "id": "ec38cd1676674d79a1f78850b2bdeade",
    "idx": 15,
    "time": "2020-11-13T17:52:55.469Z",
    "type": "execution"
   },
   {
    "id": "ec38cd1676674d79a1f78850b2bdeade",
    "time": "2020-11-13T17:52:55.543Z",
    "type": "completion"
   },
   {
    "code": "# TEST\nlen(first_ham) > 0 and first_ham[:0] == ''",
    "id": "91fb9a6b7c524dd1b4ebaf00adfee1e0",
    "idx": 16,
    "time": "2020-11-13T17:52:55.714Z",
    "type": "execution"
   },
   {
    "id": "91fb9a6b7c524dd1b4ebaf00adfee1e0",
    "time": "2020-11-13T17:52:55.787Z",
    "type": "completion"
   },
   {
    "code": "# TEST\nlen(first_spam) > 0 and first_spam[:0] == ''",
    "id": "bc8e6529a6634c3abc2fbbdeb59cf56a",
    "idx": 17,
    "time": "2020-11-13T17:52:55.830Z",
    "type": "execution"
   },
   {
    "id": "bc8e6529a6634c3abc2fbbdeb59cf56a",
    "time": "2020-11-13T17:52:55.908Z",
    "type": "completion"
   },
   {
    "code": "# HIDDEN TEST\noriginal_training_data.loc[original_training_data['spam'] == 0, 'email'].iloc[0] in first_ham",
    "id": "72db8567d04348308e6661ab83085607",
    "idx": 18,
    "time": "2020-11-13T17:52:55.952Z",
    "type": "execution"
   },
   {
    "id": "72db8567d04348308e6661ab83085607",
    "time": "2020-11-13T17:52:56.024Z",
    "type": "completion"
   },
   {
    "code": "# HIDDEN TEST\noriginal_training_data.loc[original_training_data['spam'] == 1, 'email'].iloc[0] in first_spam",
    "id": "3618a1ff098948808c2c917f942d36aa",
    "idx": 19,
    "time": "2020-11-13T17:52:56.076Z",
    "type": "execution"
   },
   {
    "id": "3618a1ff098948808c2c917f942d36aa",
    "time": "2020-11-13T17:52:56.147Z",
    "type": "completion"
   },
   {
    "code": "# This creates a 90/10 train-validation split on our labeled data\n\nfrom sklearn.model_selection import train_test_split\n\ntrain, val = train_test_split(original_training_data, test_size=0.1, random_state=42)",
    "id": "96c9ac16984747308f833d4c03e9ed53",
    "idx": 23,
    "time": "2020-11-13T17:52:56.678Z",
    "type": "execution"
   },
   {
    "id": "96c9ac16984747308f833d4c03e9ed53",
    "time": "2020-11-13T17:52:56.755Z",
    "type": "completion"
   },
   {
    "code": "def words_in_texts(words, texts):\n    '''\n    Args:\n        words (list): words to find\n        texts (Series): strings to search in\n    \n    Returns:\n        NumPy array of 0s and 1s with shape (n, p) where n is the\n        number of texts and p is the number of words.\n    '''\n    indicator_array = 1 * np.array([texts.str.contains(word) for word in words]).T # SOLUTION\n    return indicator_array",
    "id": "06639f97d5a240988a6d7dd1e0b48d8b",
    "idx": 26,
    "time": "2020-11-13T17:52:57.072Z",
    "type": "execution"
   },
   {
    "id": "06639f97d5a240988a6d7dd1e0b48d8b",
    "time": "2020-11-13T17:52:57.145Z",
    "type": "completion"
   },
   {
    "code": "# TEST\nnp.allclose(words_in_texts(['hello', 'bye', 'world'], \n                           pd.Series(['hello', 'hello worldhello'])),\n            np.array([[1, 0, 0], \n                      [1, 0, 1]]))",
    "id": "ffde5bde95224ba58486ad532fb5c28c",
    "idx": 27,
    "time": "2020-11-13T17:52:57.170Z",
    "type": "execution"
   },
   {
    "id": "ffde5bde95224ba58486ad532fb5c28c",
    "time": "2020-11-13T17:52:57.245Z",
    "type": "completion"
   },
   {
    "code": "# TEST\nnp.allclose(words_in_texts(['a', 'b', 'c', 'd', 'e', 'f', 'g'], \n                           pd.Series(['a b c d ef g', 'a', 'b', 'c', 'd e f g', 'h', 'a h'])),\n            np.array([[1,1,1,1,1,1,1], \n                      [1,0,0,0,0,0,0],\n                      [0,1,0,0,0,0,0],\n                      [0,0,1,0,0,0,0],\n                      [0,0,0,1,1,1,1],\n                      [0,0,0,0,0,0,0],\n                      [1,0,0,0,0,0,0]]))",
    "id": "e3e6cf9be8e743fba0049d314831aebf",
    "idx": 28,
    "time": "2020-11-13T17:52:57.405Z",
    "type": "execution"
   },
   {
    "id": "e3e6cf9be8e743fba0049d314831aebf",
    "time": "2020-11-13T17:52:57.482Z",
    "type": "completion"
   },
   {
    "code": "from IPython.display import display, Markdown\ndf = pd.DataFrame({\n    'word_1': [1, 0, 1, 0],\n    'word_2': [0, 1, 0, 1],\n    'type': ['spam', 'ham', 'ham', 'ham']\n})\ndisplay(Markdown(\"> Our Original DataFrame has a `type` column and some columns corresponding to words. You can think of each row as a sentence, and the value of 1 or 0 indicates the number of occurences of the word in this sentence.\"))\ndisplay(df);\ndisplay(Markdown(\"> `melt` will turn columns into entries in a variable column. Notice how `word_1` and `word_2` become entries in `variable`; their values are stored in the value column.\"))\ndisplay(df.melt(\"type\"))",
    "id": "6dd5159205814d56b783b665571c9441",
    "idx": 31,
    "time": "2020-11-13T17:52:57.979Z",
    "type": "execution"
   },
   {
    "id": "6dd5159205814d56b783b665571c9441",
    "time": "2020-11-13T17:52:58.072Z",
    "type": "completion"
   },
   {
    "code": "train=train.reset_index(drop=True) # We must do this in order to preserve the ordering of emails to labels for words_in_texts\n\n# BEGIN SOLUTION\nsome_words = ['opportunity', 'bank', 'receive', 'dear', 'best', 'deal']\nPhi_train = words_in_texts(some_words, train['email'])\n\ndf = pd.DataFrame(data = Phi_train, columns = some_words)\ndf['label'] = train['spam']\n\nplt.figure(figsize=(12,8))\nsns.barplot(x = \"variable\", \n            y = \"value\", \n            hue = \"label\", \n            data = (df\n                    .replace({'label': \n                                {0 : 'Ham', \n                                 1 : 'Spam'}})\n                    .melt('label')\n                    .groupby(['label', 'variable'])\n                    .mean()\n                    .reset_index()))\n\nplt.xlabel('Words')\nplt.ylabel('Proportion of Emails')\nplt.legend(title = \"\")\nplt.title(\"Frequency of Words in Spam/Ham Emails\")\nplt.tight_layout()\nplt.show()\n# END SOLUTION",
    "id": "cc68f68e069e48788281a1bc9531d83d",
    "idx": 33,
    "time": "2020-11-13T17:52:58.316Z",
    "type": "execution"
   },
   {
    "code": "# BEGIN SOLUTION\ntmp = train.copy()\ntmp['length'] = tmp['email'].str.len()\nplt.figure(figsize=(10, 6))\nsns.distplot(tmp.loc[tmp['spam'] == 0, 'length'],hist=False, label='Ham')\nsns.distplot(tmp.loc[tmp['spam'] == 1, 'length'],hist=False, label='Spam')\nplt.title('Distribution of Email Body Length')\nplt.xlabel('Length of email body')\nplt.ylabel('Distribution')\nplt.xlim((0,50000))\nplt.tight_layout()\nplt.legend();\n# END SOLUTION\nplt.savefig('training_conditional_densities.png')",
    "id": "26273859df904aecb41ec51ef7d9885e",
    "idx": 36,
    "time": "2020-11-13T17:52:58.861Z",
    "type": "execution"
   },
   {
    "id": "cc68f68e069e48788281a1bc9531d83d",
    "time": "2020-11-13T17:52:59.050Z",
    "type": "completion"
   },
   {
    "id": "26273859df904aecb41ec51ef7d9885e",
    "time": "2020-11-13T17:52:59.571Z",
    "type": "completion"
   },
   {
    "code": "some_words = ['drug', 'bank', 'prescription', 'memo', 'private']\n\nX_train = words_in_texts(some_words, train['email']) # SOLUTION\nY_train = np.array(train['spam']) # SOLUTION\n\nX_train[:5], Y_train[:5]",
    "id": "62fb90ac758448c48db77978df497ae6",
    "idx": 39,
    "time": "2020-11-13T17:53:00.275Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nX_train.shape == (7513, 5)",
    "id": "b365b5d2dcef401a8defa85c6f598d02",
    "idx": 40,
    "time": "2020-11-13T17:53:00.406Z",
    "type": "execution"
   },
   {
    "id": "62fb90ac758448c48db77978df497ae6",
    "time": "2020-11-13T17:53:00.493Z",
    "type": "completion"
   },
   {
    "id": "b365b5d2dcef401a8defa85c6f598d02",
    "time": "2020-11-13T17:53:00.498Z",
    "type": "completion"
   },
   {
    "code": "# TEST\nnp.array_equal(np.unique(X_train), np.array([0, 1])) # X matrix should consist of only 0 or 1",
    "id": "47d34afb59514919ba683fa0c888bde4",
    "idx": 41,
    "time": "2020-11-13T17:53:00.534Z",
    "type": "execution"
   },
   {
    "id": "47d34afb59514919ba683fa0c888bde4",
    "time": "2020-11-13T17:53:00.605Z",
    "type": "completion"
   },
   {
    "code": "# TEST\nnp.array_equal(np.unique(Y_train), np.array([0, 1])) # y vector should consist of only 0 or 1",
    "id": "e8cd7ec14d974ad9b019227069f6145d",
    "idx": 42,
    "time": "2020-11-13T17:53:00.660Z",
    "type": "execution"
   },
   {
    "id": "e8cd7ec14d974ad9b019227069f6145d",
    "time": "2020-11-13T17:53:00.733Z",
    "type": "completion"
   },
   {
    "code": "from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(solver = 'lbfgs') # SOLUTION\nmodel.fit(X_train, Y_train) # SOLUTION\n\ntraining_accuracy = model.score(X_train, Y_train) # SOLUTION\nprint(\"Training Accuracy: \", training_accuracy)",
    "id": "d499255244534699864b5465ce019b5d",
    "idx": 44,
    "time": "2020-11-13T17:53:00.942Z",
    "type": "execution"
   },
   {
    "id": "d499255244534699864b5465ce019b5d",
    "time": "2020-11-13T17:53:01.061Z",
    "type": "completion"
   },
   {
    "code": "# TEST\ntraining_accuracy > 0.72",
    "id": "fef1c6cf1e4a4d3c88f2b4953c55e80d",
    "idx": 45,
    "time": "2020-11-13T17:53:01.501Z",
    "type": "execution"
   },
   {
    "id": "fef1c6cf1e4a4d3c88f2b4953c55e80d",
    "time": "2020-11-13T17:53:01.569Z",
    "type": "completion"
   },
   {
    "code": "zero_predictor_fp = 0 # SOLUTION\nzero_predictor_fn = sum(Y_train == 1) # SOLUTION\nzero_predictor_fp, zero_predictor_fn",
    "id": "0cffaab89bf348f189d092a3bfa1ea4d",
    "idx": 49,
    "time": "2020-11-13T17:53:02.485Z",
    "type": "execution"
   },
   {
    "id": "0cffaab89bf348f189d092a3bfa1ea4d",
    "time": "2020-11-13T17:53:02.576Z",
    "type": "completion"
   },
   {
    "code": "# TEST\nzero_predictor_fp >= 0",
    "id": "564c1eaebae947b69e6800e2ae98ba16",
    "idx": 50,
    "time": "2020-11-13T17:53:02.634Z",
    "type": "execution"
   },
   {
    "id": "564c1eaebae947b69e6800e2ae98ba16",
    "time": "2020-11-13T17:53:02.712Z",
    "type": "completion"
   },
   {
    "code": "# TEST\nzero_predictor_fn >= 0",
    "id": "8a79d709c33749518cf44392623bb119",
    "idx": 51,
    "time": "2020-11-13T17:53:02.760Z",
    "type": "execution"
   },
   {
    "id": "8a79d709c33749518cf44392623bb119",
    "time": "2020-11-13T17:53:02.832Z",
    "type": "completion"
   },
   {
    "code": "# HIDDEN TEST\nnp.isclose(zero_predictor_fp, 0)",
    "id": "d6ef40aabfde4fd9afc5e0ae8a2f9a56",
    "idx": 52,
    "time": "2020-11-13T17:53:02.881Z",
    "type": "execution"
   },
   {
    "id": "d6ef40aabfde4fd9afc5e0ae8a2f9a56",
    "time": "2020-11-13T17:53:02.953Z",
    "type": "completion"
   },
   {
    "code": "# HIDDEN TEST\nzero_predictor_fn == 1918",
    "id": "dd9b240252164f86b72fb4a1a40682d4",
    "idx": 53,
    "time": "2020-11-13T17:53:03.012Z",
    "type": "execution"
   },
   {
    "id": "dd9b240252164f86b72fb4a1a40682d4",
    "time": "2020-11-13T17:53:03.092Z",
    "type": "completion"
   },
   {
    "code": "zero_predictor_acc = np.mean(Y_train == 0) # SOLUTION\nzero_predictor_recall = 0 # SOLUTION\nzero_predictor_acc, zero_predictor_recall",
    "id": "84771e6a5b2d4591884e110cd14df188",
    "idx": 55,
    "time": "2020-11-13T17:53:03.505Z",
    "type": "execution"
   },
   {
    "id": "84771e6a5b2d4591884e110cd14df188",
    "time": "2020-11-13T17:53:03.572Z",
    "type": "completion"
   },
   {
    "code": "# TEST\nzero_predictor_acc >= 0",
    "id": "060a99ebe6ce4a5086717f109d2e7dca",
    "idx": 56,
    "time": "2020-11-13T17:53:03.631Z",
    "type": "execution"
   },
   {
    "id": "060a99ebe6ce4a5086717f109d2e7dca",
    "time": "2020-11-13T17:53:03.702Z",
    "type": "completion"
   },
   {
    "code": "# TEST\nzero_predictor_recall >= 0",
    "id": "16d18caac2104e4caf93f13a65241938",
    "idx": 57,
    "time": "2020-11-13T17:53:03.762Z",
    "type": "execution"
   },
   {
    "id": "16d18caac2104e4caf93f13a65241938",
    "time": "2020-11-13T17:53:03.831Z",
    "type": "completion"
   },
   {
    "code": "# HIDDEN TEST\nnp.isclose(zero_predictor_acc, 0.7447091707706642)",
    "id": "be360f140fc546fd8b086891e6614438",
    "idx": 58,
    "time": "2020-11-13T17:53:04.126Z",
    "type": "execution"
   },
   {
    "id": "be360f140fc546fd8b086891e6614438",
    "time": "2020-11-13T17:53:04.196Z",
    "type": "completion"
   },
   {
    "code": "# HIDDEN TEST\nnp.isclose(zero_predictor_recall, 0)",
    "id": "34130958f5e14ea2aa8627842fa37faf",
    "idx": 59,
    "time": "2020-11-13T17:53:04.243Z",
    "type": "execution"
   },
   {
    "id": "34130958f5e14ea2aa8627842fa37faf",
    "time": "2020-11-13T17:53:04.312Z",
    "type": "completion"
   },
   {
    "code": "# BEGIN SOLUTION NO PROMPT\nY_train_hat = model.predict(X_train)\n\nTP = sum((Y_train_hat == Y_train) & (Y_train_hat == 1))\nTN = sum((Y_train_hat == Y_train) & (Y_train_hat == 0))\nFP = sum((Y_train_hat != Y_train) & (Y_train_hat == 1))\nFN = sum((Y_train_hat != Y_train) & (Y_train_hat == 0))\n# END SOLUTION\nlogistic_predictor_precision = TP / (TP + FP) # SOLUTION\nlogistic_predictor_recall = TP / (TP + FN) # SOLUTION\nlogistic_predictor_far = FP / (FP + TN) # SOLUTION",
    "id": "69a4c98c4a1e45449c52800df7da21d9",
    "idx": 63,
    "time": "2020-11-13T17:53:04.794Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nlogistic_predictor_precision >= 0",
    "id": "4fa2f8b900fa476f8ec08629fe9787aa",
    "idx": 64,
    "time": "2020-11-13T17:53:04.910Z",
    "type": "execution"
   },
   {
    "id": "69a4c98c4a1e45449c52800df7da21d9",
    "time": "2020-11-13T17:53:05.010Z",
    "type": "completion"
   },
   {
    "id": "4fa2f8b900fa476f8ec08629fe9787aa",
    "time": "2020-11-13T17:53:05.020Z",
    "type": "completion"
   },
   {
    "code": "# TEST\nlogistic_predictor_recall >= 0",
    "id": "f169fcb087ce4e1d87d7cada8bfc94a6",
    "idx": 65,
    "time": "2020-11-13T17:53:05.035Z",
    "type": "execution"
   },
   {
    "id": "f169fcb087ce4e1d87d7cada8bfc94a6",
    "time": "2020-11-13T17:53:05.111Z",
    "type": "completion"
   },
   {
    "code": "# TEST\nlogistic_predictor_far >= 0",
    "id": "ce0f25f5620849cc9a33d61b84e5caf6",
    "idx": 66,
    "time": "2020-11-13T17:53:05.173Z",
    "type": "execution"
   },
   {
    "id": "ce0f25f5620849cc9a33d61b84e5caf6",
    "time": "2020-11-13T17:53:05.250Z",
    "type": "completion"
   },
   {
    "code": "# HIDDEN TEST\nnp.isclose(logistic_predictor_precision, 0.6422287390029325)",
    "id": "2da6368a3815416ba07c4cbfaf0533d5",
    "idx": 67,
    "time": "2020-11-13T17:53:05.299Z",
    "type": "execution"
   },
   {
    "id": "2da6368a3815416ba07c4cbfaf0533d5",
    "time": "2020-11-13T17:53:05.379Z",
    "type": "completion"
   },
   {
    "code": "# HIDDEN TEST\nnp.isclose(logistic_predictor_recall, 0.11418143899895725)",
    "id": "1f49e01547f844499992699a3f8437d4",
    "idx": 68,
    "time": "2020-11-13T17:53:05.414Z",
    "type": "execution"
   },
   {
    "id": "1f49e01547f844499992699a3f8437d4",
    "time": "2020-11-13T17:53:05.484Z",
    "type": "completion"
   },
   {
    "code": "# HIDDEN TEST\nnp.isclose(logistic_predictor_far, 0.021805183199285077)",
    "id": "a86939b61fe7402b9bd562eda7adcb8a",
    "idx": 69,
    "time": "2020-11-13T17:53:05.535Z",
    "type": "execution"
   },
   {
    "id": "a86939b61fe7402b9bd562eda7adcb8a",
    "time": "2020-11-13T17:53:05.607Z",
    "type": "completion"
   },
   {
    "code": "# Write your description (2-3 sentences) as a comment here:\n# \n#\n#\n\n# Write the code to generate your visualization here:\n# BEGIN SOLUTION\nplt.plot([1, 3, 5]) # This is a dummy plot, not a real example of a solution\n# END SOLUTION",
    "id": "f9dd9c1872054b3aabcc6cde9965ad16",
    "idx": 79,
    "time": "2020-11-13T17:53:06.802Z",
    "type": "execution"
   },
   {
    "code": "from sklearn.metrics import roc_curve\n\n# Note that you'll want to use the .predict_proba(...) method for your classifier\n# instead of .predict(...) so you get probabilities, not classes\n\n# BEGIN SOLUTION\nstaff_words = ['body', 'click', 'please', 'base64', '2002', 'html', 'subscribed',\n               'wrote', 'mortgage', 'align3dcenterfont', 'dear', 'br', 'width10img',\n               'divfont', 'im', 'receive', 'list', 'tags', 'web', 'base64', 'click',\n               'body', 'please', 'money', 'offer', 'receive', 'contact', 'free',\n               'tr', 'removed', 'remove', 'html', 'font', 'form',\n               'credit', 'business', 'div']\n\nX_train_2 = words_in_texts(staff_words, train['email'])\n\nstaff_model = LogisticRegression(solver = 'lbfgs')\nstaff_model.fit(X_train_2, Y_train)\n\nprint('accuracy: ', staff_model.score(X_train_2, Y_train))\n\nY_predict = staff_model.predict_proba(X_train_2)[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_train, Y_predict)\nwith sns.axes_style(\"white\"):\n    plt.plot(fpr, tpr)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.show()\n# END SOLUTION",
    "id": "73428c5ed83743b199ed18dc26f18be3",
    "idx": 81,
    "time": "2020-11-13T17:53:07.068Z",
    "type": "execution"
   },
   {
    "id": "f9dd9c1872054b3aabcc6cde9965ad16",
    "time": "2020-11-13T17:53:07.170Z",
    "type": "completion"
   },
   {
    "code": "test_predictions = staff_model.predict(words_in_texts(staff_words, test['email'])) # SOLUTION",
    "id": "665d4320ee5a44fd8e5fc2cc55ecd777",
    "idx": 83,
    "time": "2020-11-13T17:53:07.542Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nisinstance(test_predictions, np.ndarray) # must be ndarray of predictions",
    "id": "0dd28fcc619746e4831ad536f8956532",
    "idx": 84,
    "time": "2020-11-13T17:53:07.669Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nnp.array_equal(np.unique(test_predictions), np.array([0, 1])) # must be binary labels (0 or 1) and not probabilities",
    "id": "6ccaddb5360c45609af4d3b6991ca741",
    "idx": 85,
    "time": "2020-11-13T17:53:07.805Z",
    "type": "execution"
   },
   {
    "code": "# TEST\nlen(test_predictions) == 1000 # must be the right number of predictions",
    "id": "e8c8f77a5758407293ae0b201063880f",
    "idx": 86,
    "time": "2020-11-13T17:53:07.949Z",
    "type": "execution"
   },
   {
    "code": "from datetime import datetime\n\n# Assuming that your predictions on the test set are stored in a 1-dimensional array called\n# test_predictions. Feel free to modify this cell as long you create a CSV in the right format.\n\n# Construct and save the submission:\nsubmission_df = pd.DataFrame({\n    \"Id\": test['id'], \n    \"Class\": test_predictions,\n}, columns=['Id', 'Class'])\ntimestamp = datetime.isoformat(datetime.now()).split(\".\")[0]\nsubmission_df.to_csv(\"submission_{}.csv\".format(timestamp), index=False)\n\nprint('Created a CSV file: {}.'.format(\"submission_{}.csv\".format(timestamp)))\nprint('You may now upload this CSV file to Gradescope for scoring.')",
    "id": "aa5bacff8532404a8471d14006215349",
    "idx": 88,
    "time": "2020-11-13T17:53:08.231Z",
    "type": "execution"
   },
   {
    "id": "73428c5ed83743b199ed18dc26f18be3",
    "time": "2020-11-13T17:53:08.462Z",
    "type": "completion"
   },
   {
    "id": "665d4320ee5a44fd8e5fc2cc55ecd777",
    "time": "2020-11-13T17:53:08.523Z",
    "type": "completion"
   },
   {
    "id": "0dd28fcc619746e4831ad536f8956532",
    "time": "2020-11-13T17:53:08.540Z",
    "type": "completion"
   },
   {
    "id": "6ccaddb5360c45609af4d3b6991ca741",
    "time": "2020-11-13T17:53:08.558Z",
    "type": "completion"
   },
   {
    "id": "e8c8f77a5758407293ae0b201063880f",
    "time": "2020-11-13T17:53:08.585Z",
    "type": "completion"
   },
   {
    "id": "aa5bacff8532404a8471d14006215349",
    "time": "2020-11-13T17:53:08.632Z",
    "type": "completion"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "11053374b51b9ba60f923be4d4dab668503bcfab244429af3685fde87844d07a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
